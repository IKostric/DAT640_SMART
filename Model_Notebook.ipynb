{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json, os, pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from util.es import ES\n",
    "from util.io import load_dict_from_json\n",
    "from util.parse_dbpedia import get_type_weights\n",
    "from smart_dataset.evaluation.dbpedia.evaluate import load_type_hierarchy, evaluate, get_type_path\n",
    "\n",
    "from QPC import QPC_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            classifier: An instance of scikit-learn regressor.\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"Trains an LTR model.\n",
    "        \n",
    "        Arguments:\n",
    "            X: Features of training instances.\n",
    "            y: Relevance assessments of training instances.\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"Predicts relevance labels and rank documents for a given query.\n",
    "        \n",
    "        Arguments:\n",
    "            ft: A list of feature vectors for query-document pairs.\n",
    "            doc_ids: A list of document ids.\n",
    "        Returns:\n",
    "            List of tuples, each consisting of document ID and predicted relevance label.\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in sort_indices:\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline(dataset='train', n_list=[5, 10, 20, 50, 100]):\n",
    "    baseline = []\n",
    "    for similarity in ['BM25', 'LM']:\n",
    "        for n in n_list:\n",
    "            results = {key: {k:v for k,v in val} for key,val in ES('EC', similarity).generate_baseline_scores(dataset, n).items()}\n",
    "            baseline.append(results)\n",
    "        results = {key: {k:v for k,v in val} for key,val in ES('TC', similarity).generate_baseline_scores(dataset).items()}\n",
    "        baseline.append(results)\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES = get_type_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature files\n",
    "FAMILY = load_dict_from_json('type_hierarchy_features.json')\n",
    "T_LENGTH = load_dict_from_json('type_length_features.json')\n",
    "T_LABEL = load_dict_from_json('type_label_idf_features.json')\n",
    "Q_T_IDF = load_dict_from_json('type_query_idf_features.json')\n",
    "q_ids = load_dict_from_json('q_id_list.json')\n",
    "with open(os.path.join('data','Q_T_features'),'rb') as f:\n",
    "    qt_features = pickle.load(f)\n",
    "with open(os.path.join('data','lacking_J_term_types'),'rb') as f:\n",
    "    lacking_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type hierarchy from ./smart_dataset/evaluation/dbpedia/dbpedia_types.tsv... 760 types loaded (max depth: 7)\n"
     ]
    }
   ],
   "source": [
    "type_hierarchy, max_depth = load_type_hierarchy('./smart_dataset/evaluation/dbpedia/dbpedia_types.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q_T_features(qid,t,lft=qt_features,lqs=q_ids,s='train',lacking=lacking_list):\n",
    "    '''\n",
    "    Function for extracting SIMAGGR and JTERMS features for\n",
    "    query-type pairs.\n",
    "    Will use average values if type not in the aggregated\n",
    "    type docuemnt.\n",
    "    '''\n",
    "    s = 'val' if s == 'validation' else s\n",
    "    q_idx = q_ids[s][qid]\n",
    "    t_label = t[4:]\n",
    "\n",
    "    return([lft[t_label][s]['JTERMS'][q_idx],\\\n",
    "            (lft[t_label][s]['SIMAGGR'][q_idx][0]+1)/2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(qid, t, dataset = 'train'):\n",
    "    \"\"\"\n",
    "    Returns features to use in advanced model prediction.\n",
    "    \"\"\"        \n",
    "    # Add baseline features in following order:\n",
    "    # EC_BM, EC_LM, TC_BM, TC_LM\n",
    "    features = [es.get(qid, {}).get(t, 0) for es in BASELINE[dataset]]\n",
    "    \n",
    "    # add ENTITIES\n",
    "    features.append(ENTITIES.get(t, 0))\n",
    "    \n",
    "    # add type family features\n",
    "    if t in FAMILY.keys():\n",
    "        features.append(FAMILY[t]['depth'])\n",
    "        features.append(len(FAMILY[t]['siblings']))\n",
    "        features.append(len(FAMILY[t]['children'])) \n",
    "    else:\n",
    "        print('type: {} not in hierarchy list'.format(t))\n",
    "        features +=[0,0,0]\n",
    "    \n",
    "    # add type length\n",
    "    if t in T_LENGTH.keys():\n",
    "        features.append(T_LENGTH[t])\n",
    "    else:\n",
    "    #    print('type: {} not in type length list'.format(t))\n",
    "        features.append(0) \n",
    "        \n",
    "    # add IDF label features\n",
    "    if t in T_LABEL.keys():\n",
    "        for f in T_LABEL[t]['X'].values():\n",
    "            features.append(f) \n",
    "    else:\n",
    "  #      print('type: {} not in labels list'.format(t))\n",
    "        features += [0]*4\n",
    "    \n",
    "        # add query type IDF features\n",
    "    if t in Q_T_IDF.keys():\n",
    "        for f in Q_T_IDF[t]['X'].values():\n",
    "            features.append(f) \n",
    "    else:\n",
    "   #     print('type: {} not in labels list'.format(t))\n",
    "        features += [0]*4\n",
    "    \n",
    "    # add Q-T features\n",
    "    features.extend(get_Q_T_features(qid,t,s=dataset))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ltr_training_data(queries):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        types = set([*query['type'], *list(itertools.chain.from_iterable([list(es.get(query['id'], {}).keys()) for es in BASELINE['train']]))])\n",
    "        for t in types:\n",
    "            X.append(extract_features(query['id'], t))\n",
    "            y.append(1 if t in query['type'] else 0)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #loading pretrained model\n",
    "    with open(os.path.join('saved_models','ltr_unlim_2'),'rb') as f:\n",
    "        ltr = pickle.load(f)\n",
    "    print('Loaded pretrained model')\n",
    "except:\n",
    "    print('Unable to load model. Training...')\n",
    "    X_train, y_train = prepare_ltr_training_data(queries['train'])\n",
    "    # Instantiate an scikit-learn regression model, `clf`.\n",
    "    clf = RandomForestRegressor(max_depth = 2, n_estimators=1000)\n",
    "\n",
    "    # Instantiate PointWiseLTRModel.\n",
    "    ltr = PointWiseLTRModel(clf)\n",
    "    ltr._train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rankings(baseline, ltr, queries, dataset='train'):\n",
    "    test_rankings = {}\n",
    "    if dataset not in baseline:\n",
    "        baseline[dataset] = get_baseline(dataset)\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        if query['id'] in baseline[dataset][0]:\n",
    "            types = list(set([*list(itertools.chain.from_iterable([list(es.get(query['id'], {}).keys()) for es in BASELINE[dataset]]))]))\n",
    "            #types = list(type_hierarchy.keys())\n",
    "            features = [extract_features(query['id'], t, dataset) for t in types]\n",
    "            if len(types)>0:\n",
    "                test_rankings[query['id']] = ltr.rank(features, types)\n",
    "            else:\n",
    "                test_rankings[query['id']] = []\n",
    "        else:\n",
    "            test_rankings[query['id']] = []\n",
    "        \n",
    "    return test_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_rankings(baseline, model_nr, queries, dataset='test', n_list=[20]):\n",
    "    test_rankings = {}\n",
    "    if dataset not in baseline:\n",
    "        baseline[dataset] = get_baseline(dataset, n_list)\n",
    "        \n",
    "    for i, query in enumerate(queries):\n",
    "        if query['id'] in baseline[dataset][model_nr]:\n",
    "            test_rankings[query['id']] = [(t,s) for t,s in baseline[dataset][model_nr][query['id']].items()]\n",
    "        else:\n",
    "            test_rankings[query['id']] = []\n",
    "    return test_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(dataset, type_hierarchy):\n",
    "    ground_truth = {}\n",
    "    for query in dataset:\n",
    "        ID = query['id']\n",
    "\n",
    "        ground_truth_category = query['category']\n",
    "        ground_truth_type = [t for t in query['type'] if t in type_hierarchy]\n",
    "\n",
    "        if not ground_truth_type:\n",
    "            continue\n",
    "\n",
    "        ground_truth[ID] = {\n",
    "            'category': ground_truth_category,\n",
    "            'type': ground_truth_type\n",
    "        }\n",
    "        \n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_outputs(results, queries, type_hierarchy):\n",
    "    system_output = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        ID = query['id']\n",
    "\n",
    "        system_output_type = [t for t,s in results[ID] if t in type_hierarchy] if ID in results else []\n",
    "        #system_output_type = get_type_path(system_output_type[0], type_hierarchy) if system_output_type else []\n",
    "\n",
    "        system_output[ID] = {\n",
    "            'category': 'resource',\n",
    "            'type': system_output_type\n",
    "        }\n",
    "        \n",
    "    return system_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_other(queries, pred, classes):\n",
    "    '''\n",
    "    Function for formatting non-resource queries.\n",
    "    '''\n",
    "    system_output = {}\n",
    "    for q in queries:\n",
    "        ID = q['id']\n",
    "        assert pred[ID] != 0 #checking no resource predicted queries\n",
    "        if pred[ID] in [1,2,3]: #setting correct category and type\n",
    "            category = 'literal'\n",
    "            out_type = classes[pred[ID]]\n",
    "        else: #boolean\n",
    "            category = classes[pred[ID]]\n",
    "            out_type = classes[pred[ID]]\n",
    "        \n",
    "        system_output[ID] = {\n",
    "            'category': category,\n",
    "            'type': [out_type]\n",
    "        }\n",
    "        \n",
    "    return system_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate Baselines (takes a while, uncomment)\n",
    "#BASELINE_20 = {'test':get_baseline(dataset='test',n_list=[20])}\n",
    "#BASELINE = {'train':get_baseline(dataset='train'), 'validation':get_baseline(dataset='validation'),'test':get_baseline(dataset='test')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading queries\n",
    "queries = {}\n",
    "for name in ['train', 'validation', 'test']:\n",
    "    dataset = load_dict_from_json(f'{name}_set_fixed.json')\n",
    "    #queries[name] = [q for q in dataset if q['category'] == 'resource']\n",
    "    queries[name] = [q for q in dataset] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_result_string(ev, l):\n",
    "    '''\n",
    "    Prepares result string for printing.\n",
    "    '''\n",
    "    return '\\nResults for\\n{}\\n=====================\\nCategory prediction\\n   Accuracy:  {}\\nType ranking\\n   NDCG@5:    {}\\n   NDCG@10:   {}\\n'.format(l,ev['Accuracy'],ev['NDCG5'],ev['NDCG10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 1625 questions)\n",
      "  Accuracy: 0.994\n",
      "Type ranking (based on 1349 questions)\n",
      "  NDCG@5:  0.458\n",
      "  NDCG@10: 0.479\n",
      "\n",
      "\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 1625 questions)\n",
      "  Accuracy: 0.994\n",
      "Type ranking (based on 1349 questions)\n",
      "  NDCG@5:  0.456\n",
      "  NDCG@10: 0.468\n",
      "\n",
      "\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 1625 questions)\n",
      "  Accuracy: 0.994\n",
      "Type ranking (based on 1349 questions)\n",
      "  NDCG@5:  0.437\n",
      "  NDCG@10: 0.456\n",
      "\n",
      "\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 1625 questions)\n",
      "  Accuracy: 0.994\n",
      "Type ranking (based on 1349 questions)\n",
      "  NDCG@5:  0.468\n",
      "  NDCG@10: 0.473\n",
      "\n",
      "\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 1625 questions)\n",
      "  Accuracy: 0.994\n",
      "Type ranking (based on 1349 questions)\n",
      "  NDCG@5:  0.630\n",
      "  NDCG@10: 0.626\n"
     ]
    }
   ],
   "source": [
    "#cell for running test\n",
    "labels = ['Baseline EC k=20 BM25', 'Baseline EC k=20 LM', 'Baseline TC BM25', 'Baseline TC LM', 'Advanced Model']\n",
    "name = 'test'\n",
    "text_file = ''\n",
    "\n",
    "#Initializing pretrained query category classifier, denoted Step 1 in the report\n",
    "categorizer = QPC_model()\n",
    "categorizer.model()\n",
    "\n",
    "#predicting category + potential 'literal' type\n",
    "prediction = categorizer.predict(queries[name])\n",
    "classes = categorizer.classes\n",
    "res_queries = []\n",
    "other_queries = []\n",
    "other_pred = []\n",
    "for query in queries[name]:\n",
    "    qID = query['id']\n",
    "    if prediction[qID] == 0:\n",
    "        res_queries.append(query)\n",
    "    else:\n",
    "        other_queries.append(query)\n",
    "\n",
    "#handle non-resource queries separately\n",
    "other_output = format_other(other_queries, prediction, classes)\n",
    "other_ground_truth = get_ground_truth(other_queries, ['boolean','date','number','string'])\n",
    "\n",
    "for m in [0,1,2,3,4]:\n",
    "    \n",
    "    if m<4: #for the baseline models\n",
    "        results = baseline_rankings(BASELINE_20, m, res_queries, dataset='test', n_list=[20])\n",
    "    else: #for advanced model\n",
    "        results = get_rankings(BASELINE, ltr, queries[name], dataset=name)\n",
    "    system_output = format_outputs(results, res_queries, type_hierarchy)\n",
    "    ground_truth = get_ground_truth(res_queries, type_hierarchy)\n",
    "    system_output.update(other_output)\n",
    "    ground_truth.update(other_ground_truth)\n",
    "    ev_ret = evaluate(system_output, ground_truth, type_hierarchy, max_depth)\n",
    "    \n",
    "    text_file += prepare_result_string(ev_ret,labels[m])\n",
    "\n",
    "\n",
    "with open('test_results_full_model.txt','w') as f:\n",
    "        f.write(text_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 932 questions)\n",
      "  Accuracy: 1.000\n",
      "Type ranking (based on 932 questions)\n",
      "  NDCG@5:  0.457\n",
      "  NDCG@10: 0.460\n"
     ]
    }
   ],
   "source": [
    "# Set which query set to use for evaluation\n",
    "name = 'validation'\n",
    "\n",
    "# Evaluate\n",
    "results = get_rankings(BASELINE,ltr, queries[name], name)\n",
    "system_output = format_outputs(results, queries[name], type_hierarchy)\n",
    "ground_truth = get_ground_truth(queries[name], type_hierarchy)\n",
    "ev_ret = evaluate(system_output, ground_truth, type_hierarchy, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
