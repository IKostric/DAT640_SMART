{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from json import JSONDecodeError\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from parse_dbpedia import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'BIEBER',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'nN32_N-8T1OLuu3AJOSr2g',\n",
       " 'version': {'number': '7.9.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': '083627f112ba94dffc1232e8b42b73492789ef91',\n",
       "  'build_date': '2020-09-01T21:22:21.964974Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, index, query, field='body'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index. \n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed. \n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index. \n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index,\n",
    "                         body={\n",
    "                             'query': {\n",
    "                                 'match': {\n",
    "                                     field: t['token']\n",
    "                                 }\n",
    "                             }\n",
    "                         },\n",
    "                         _source=False,\n",
    "                         size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "def baseline_retrieval(es, index_name, query, k=100):\n",
    "    \"\"\"Performs baseline retrival on index.\n",
    "    \n",
    "    Arguments:\n",
    "        index_name: A string of text.\n",
    "        query: A string of text, space separated terms.\n",
    "        k: An integer.\n",
    "        \n",
    "    Returns:\n",
    "        A list of entity IDs as strings, up to k of them, in descending order of scores.\n",
    "    \"\"\"\n",
    "\n",
    "    hits = es.search(index=index_name, q=query, _source=True,\n",
    "                     size=k)['hits']['hits']\n",
    "    return [(doc['_score'], doc['_source']['types']) for doc in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = 'entity_centric_bm25_long_abstract'\n",
    "INDEX_SETTINGS = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'body': {\n",
    "                'type': 'text',\n",
    "                'term_vector': 'yes',\n",
    "                'analyzer': 'english'\n",
    "            },\n",
    "            'types': {\n",
    "                'type': 'text',\n",
    "                'term_vector': 'yes',\n",
    "                'analyzer': 'english'\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(es, index):\n",
    "    if es.indices.exists(INDEX_NAME):\n",
    "        es.indices.delete(index=INDEX_NAME)\n",
    "\n",
    "    es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)\n",
    "    \n",
    "def get_data(index, doc):\n",
    "    num_docs = len(doc) // 10\n",
    "    for i, (doc_id, body) in enumerate(doc.items()):\n",
    "        yield {'_index': index, '_id': doc_id, '_source': body}\n",
    "        if i % num_docs == 0:\n",
    "            print('{}% done'.format((i // num_docs)*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num entities with types:  4767652\n",
      "Num entities with data:  4935279\n"
     ]
    }
   ],
   "source": [
    "ontology = get_ontology_tree()\n",
    "doc_entity = get_entity_docs()\n",
    "entity_data = get_entity_data()\n",
    "for entity, body in doc_entity.items():\n",
    "    ancestors = []\n",
    "    for entity_type in body['types'].split():\n",
    "        ancestors.extend(get_ancestors(ontology, entity_type))\n",
    "    doc_entity[entity]['types'] = ' '.join(set(ancestors))\n",
    "    doc_entity[entity]['body'] += ' ' + entity_data.get(entity, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': 'Library of Alexandria The Royal Library of Alexandria or Ancient Library of Alexandria in Alexandria, Egypt, was one of the largest and most significant libraries of the ancient world. It was dedicated to the Muses, the nine goddesses of the arts. It flourished under the patronage of the Ptolemaic dynasty and functioned as a major center of scholarship from its construction in the 3rd century BC until the Roman conquest of Egypt in 30 BC, with collections of works, lecture halls, meeting rooms, and gardens. The library was part of a larger research institution called the Musaeum of Alexandria, where many of the most famous thinkers of the ancient world studied. The library was created by Ptolemy I Soter, who was a Macedonian general and the successor of Alexander the Great. Most of the books were kept as papyrus scrolls. It is unknown precisely how many such scrolls were housed at any given time, but estimates range from 40,000 to 400,000 at its height. Arguably, this library is most famous for having been burned down resulting in the loss of many scrolls and books; its destruction has become a symbol for the loss of cultural knowledge. Sources differ on who was responsible for its destruction and when it occurred. The library may in truth have suffered several fires over many years. Possible occasions for the partial or complete destruction of the Library of Alexandria include a fire set by the army of Julius Caesar in 48 BC and an attack by Aurelian in the 270s AD. After the main library was destroyed, scholars used a \\\\\"daughter library\\\\\" in a temple known as the Serapeum, located in another part of the city. According to Socrates of Constantinople, Coptic Pope Theophilus destroyed the Serapeum in AD 391, although it is not certain what it contained or if it contained any significant fraction of the documents that were in the main library. The library may have finally been destroyed during the Muslim conquest of Egypt in (or after) AD 642.',\n",
       " 'types': 'Agent ArchitecturalStructure Place EducationalInstitution Organisation Library Building'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_entity['http://dbpedia.org/resource/Library_of_Alexandria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% done\n",
      "1% done\n",
      "2% done\n",
      "3% done\n",
      "4% done\n",
      "5% done\n",
      "6% done\n",
      "7% done\n",
      "8% done\n",
      "9% done\n",
      "10% done\n"
     ]
    }
   ],
   "source": [
    "reset_index(es, INDEX_NAME)\n",
    "for success, info in parallel_bulk(es,\n",
    "                                   get_data(INDEX_NAME, doc_entity),\n",
    "                                   thread_count=12,\n",
    "                                   chunk_size=5000,\n",
    "                                   queue_size=6):\n",
    "    if not success:\n",
    "        print('A document failed:', info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4767652'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.cat.count(INDEX_NAME, params={\"format\": \"json\"})[0]['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type hierarchy from ./smart_dataset/evaluation/dbpedia/dbpedia_types.tsv... 760 types loaded (max depth: 7)\n"
     ]
    }
   ],
   "source": [
    "from smart_dataset.evaluation.dbpedia.evaluate import load_type_hierarchy, evaluate, get_type_path\n",
    "with open('./data/train_set', 'r') as f:\n",
    "    train_set = json.load(f)\n",
    "    \n",
    "type_hierarchy, max_depth = load_type_hierarchy('./smart_dataset/evaluation/dbpedia/dbpedia_types.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing question:\n",
      "{'id': 'dbpedia_9619', 'question': None, 'category': 'resource', 'type': ['dbo:Person', 'dbo:Agent']}\n",
      "\n",
      "Missing question:\n",
      "{'id': 'dbpedia_7262', 'question': None, 'category': 'resource', 'type': ['dbo:Single', 'dbo:MusicalWork', 'dbo:Work']}\n",
      "\n",
      "Missing question:\n",
      "{'id': 'dbpedia_3102', 'question': None, 'category': 'resource', 'type': ['dbo:Award']}\n",
      "\n",
      "Missing question:\n",
      "{'id': 'dbpedia_10264', 'question': None, 'category': 'resource', 'type': ['dbo:Media']}\n",
      "\n",
      "Missing question:\n",
      "{'id': 'dbpedia_12182', 'question': None, 'category': 'resource', 'type': ['dbo:Person', 'dbo:Agent']}\n",
      "\n",
      "Missing question:\n",
      "{'id': 'dbpedia_3072', 'question': None, 'category': 'resource', 'type': ['dbo:Village', 'dbo:Settlement', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']}\n",
      "\n",
      "Missing question:\n",
      "{'id': 'dbpedia_16491', 'question': None, 'category': 'resource', 'type': ['dbo:City', 'dbo:Settlement', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']}\n",
      "\n",
      "Missing question:\n",
      "{'id': 'dbpedia_17378', 'question': None, 'category': 'resource', 'type': ['dbo:Activity']}\n",
      "\n",
      "Missing question:\n",
      "{'id': 'dbpedia_12156', 'question': None, 'category': 'resource', 'type': ['dbo:Library', 'dbo:EducationalInstitution', 'dbo:Organisation', 'dbo:Agent']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "system_output = {}\n",
    "ground_truth = {}\n",
    "for item in train_set:\n",
    "    if item['category'] == 'resource':\n",
    "        if not item['question']:\n",
    "            print('Missing question:\\n{}\\n'.format(item))\n",
    "            continue\n",
    "            \n",
    "        query = analyze_query(es, INDEX_NAME, item['question'])\n",
    "        \n",
    "        scores = defaultdict(int)\n",
    "        for score, types in baseline_retrieval(es, INDEX_NAME, ' '.join(query), k):\n",
    "            for t in types.split():\n",
    "                scores['dbo:' + t] += score\n",
    "        \n",
    "        top_res = max(scores.items(), key=lambda x: x[1] if x[0] in type_hierarchy else 0\n",
    "            )[0] if len(scores) > 0 else None\n",
    "        \n",
    "        ground_truth_type = [t for t in item['type'] if t in type_hierarchy]\n",
    "        \n",
    "        system_output[item['id']] = {\n",
    "            'category': 'resource',\n",
    "            'type': [t for t in get_type_path(top_res, type_hierarchy)] if top_res else []\n",
    "        }\n",
    "        ground_truth[item['id']] = {\n",
    "            'category': item['category'],\n",
    "            'type': ground_truth_type\n",
    "        }\n",
    "        #if len(system_output) > 50:\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(system_output, ground_truth, type_hierarchy, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
